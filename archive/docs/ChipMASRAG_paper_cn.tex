\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage[UTF8,fontset=fandol]{ctex}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{mathrsfs}
\usepackage{newtxtext,newtxmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{url}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc,shapes.geometric}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{ChipMASRAG: 基于多智能体协作与RAG检索的\\超大规模芯片布局优化框架}

\author{\IEEEauthorblockN{孙可钦$^{\dagger*}$, 杨秋松$^{\dagger}$, 李明树$^{\dagger}$}
\IEEEauthorblockA{Email: sunkeqin11@mails.ucas.edu.cn, qiusong@iscas.ac.cn, mingshu@admin.iscas.ac.cn}
\IEEEauthorblockA{$^{\dagger}$中国科学院软件研究所, 北京, 中国}
\IEEEauthorblockA{$^{*}$中国科学院大学, 北京, 中国}
}

\maketitle

\begin{abstract}
超大规模芯片设计包含数十万乃至百万级模块，传统布局方法面临计算复杂度爆炸和知识复用不足的双重挑战。本文提出ChipMASRAG框架，首次将多智能体协作机制与RAG知识检索技术相结合，实现知识驱动的多智能体层次化布局优化。核心创新包括：(1)多智能体协作架构，每个分区对应一个智能体，通过协调者统一管理；(2)RAG增强的知识检索机制，协调者统一检索历史案例，结果共享给所有分区智能体；(3)知识驱动的边界协商协议，智能体间协商时参考历史案例的协商模式；(4)双重规模无关机制，通过层次化分解降低问题规模，通过多智能体并行实现训练规模无关。在ISPD 2015基准测试的12个设计上（28K-1.2M组件），ChipMASRAG实现81.25\%成功率，知识库命中时加速约63倍，平均子问题规模降至原始规模的1/35。相比ChipDRAG和DREAMPlace，在规模导向和平衡型评价中全面胜出，验证了多智能体协作与RAG检索结合的有效性。
\end{abstract}

\begin{IEEEkeywords}
芯片布局, 多智能体系统, RAG检索, 知识驱动优化, 层次化分解
\end{IEEEkeywords}

\section{引言}

\subsection{研究动机}

现代超大规模集成电路（VLSI）设计包含数十万至百万级模块，芯片布局作为物理设计的关键步骤，直接影响芯片的性能、功耗和制造成本\cite{RN47,RN61}。Kahng\cite{RN47}和Huang等\cite{RN61}系统综述了机器学习在EDA和物理设计中的应用。传统布局方法面临两个根本性挑战：首先，计算复杂度随设计规模指数增长，状态空间为$O(|S|^{|M|})$，数十万模块时计算实际上不可行；其次，现有方法缺乏历史知识复用机制，每次优化都从头开始，无法利用积累的设计经验。

现有方法主要分为三类：分析式优化方法如DREAMPlace\cite{RN92}、RePlAce\cite{RN91}通过求解非凸优化问题保证解质量，但计算复杂度超线性增长，难以扩展到百万级模块；层次化分解方法通过分治降低复杂度\cite{RN114}，但静态分区策略导致显著的边界代价，且缺乏知识复用机制；知识检索方法通过RAG技术复用历史策略\cite{RN246,RN223,RN225}，但主要关注参数生成，未解决超大规模设计的可扩展性问题。

多智能体系统（MAS）为芯片布局提供了新的建模视角。在MAS框架下，每个分区被建模为独立的智能体，具有局部自主性来优化其内部布局。智能体之间通过通信机制实现动态协作，能够协商边界模块的分配以减少跨边界连接。然而，现有多智能体方法缺乏历史知识复用，无法利用积累的设计经验加速优化过程。

这一现状揭示了芯片布局领域的核心挑战：\textit{如何在保持计算可扩展性的同时，充分利用历史知识加速优化并提升解质量？}本文提出将多智能体协作机制与RAG知识检索技术相结合，实现知识驱动的多智能体层次化布局优化。

\subsection{核心思路与贡献}

本文提出ChipMASRAG框架，将芯片布局建模为知识驱动的多智能体协作优化问题。核心思路是：通过层次化分解将超大规模设计分解为可管理的子问题，每个子问题对应一个分区智能体；协调者智能体统一执行RAG检索，从知识库中检索相似历史案例，结果共享给所有分区智能体；分区智能体通过RAG检索复用历史策略，通过边界协商协议优化边界模块分配；对于RAG未命中的新设计，采用动态规划作为兜底方案保证解质量下界。

本文的主要贡献包括四个方面。首先，提出首个将多智能体协作与RAG检索结合的芯片布局框架，将问题形式化为知识驱动的多智能体马尔可夫博弈。其次，设计RAG增强的多智能体状态空间和奖励函数，使智能体能够利用历史知识指导优化决策。第三，提出知识驱动的边界协商协议，智能体间协商时参考历史案例的协商模式，提升边界优化效果。第四，实现双重规模无关机制，通过层次化分解降低问题规模（平均1/35），通过多智能体并行实现训练规模无关，使超大规模设计从不可计算变为稳定可计算。

\section{相关工作}

\subsection{传统布局优化方法}

传统布局优化方法主要分为分析式方法和层次化方法两大类。分析式方法如DREAMPlace\cite{RN92}和RePlAce\cite{RN91}通过求解非凸优化问题来获得高质量的布局解。DREAMPlace利用GPU加速和深度学习技术实现了高效的密度驱动布局\cite{RN92}，而RePlAce通过静电类比和Nesterov梯度方法\cite{RN255}提升了收敛速度。DREAMPlace的后续工作\cite{RN98,RN270}虽然改进了时序驱动和区域约束处理，但在超大规模设计（680K+组件）上仍存在可扩展性问题。然而，这些方法的计算复杂度随设计规模呈超线性增长，难以扩展到百万级模块的超大规模设计。

层次化方法通过递归分区来分解问题复杂度，是处理大规模设计的常用策略\cite{RN114}。这类方法首先将设计划分为多个子区域，然后对每个子区域独立进行布局优化。虽然层次化方法显著降低了计算复杂度，但存在三个根本性局限：首先，静态分区策略无法根据设计特征动态调整，导致负载不均衡；其次，分区边界上的模块连接产生显著的边界代价；第三，缺乏历史知识复用机制，每次优化都从头开始。机器学习方法在物理设计中的应用\cite{RN61,RN62,RN79}尝试通过数据驱动方式优化布局，但训练成本和泛化能力的权衡仍是挑战。

\subsection{RAG与知识检索方法}

检索增强生成（RAG）技术在知识密集型任务中取得了显著成功\cite{RN246,RN228}。Lewis等\cite{RN246}首次提出RAG框架，通过检索外部知识增强语言模型的生成能力。后续工作如DynamicRAG\cite{RN223}和Shapkin等\cite{RN225}提出的动态检索增强生成进一步提升了RAG的适应性。Su等\cite{RN182}提出的DRAGIN方法基于LLM的信息需求进行动态检索，Wang等\cite{RN152}提出的RAT方法通过检索增强思维实现长序列生成中的上下文感知推理。

在EDA领域，RAG技术得到初步应用。ORAssistant\cite{RN215}提出了基于RAG的OpenROAD对话助手，ChatEDA\cite{RN157}利用大语言模型构建了EDA自主智能体。然而，这些工作主要应用于EDA工具使用和代码生成，尚未探索RAG在物理设计优化中的应用。现有RAG方法在芯片布局中的局限在于：缺乏与优化算法的深度集成，RAG检索结果主要用于参数生成而非策略优化；缺乏多智能体协作机制，无法充分利用RAG检索结果指导分布式优化。

\subsection{多智能体强化学习}

多智能体强化学习在机器人协作、自动驾驶和游戏AI等领域取得了显著成功\cite{RN219,RN218}。Cheng等\cite{RN219}提出了策略梯度布局和生成式路由神经网络，Cheng和Yan\cite{RN218}探索了联合学习解决布局和布线问题。MARL的核心优势在于能够分解复杂的全局优化问题为多个局部子问题，通过智能体间的协作达成全局目标。然而，MARL也面临若干理论和实践挑战，包括非平稳性、信用分配和通信开销。

在EDA领域，最近出现了一些基于智能体的探索性工作。PCBAgent\cite{RN217}提出了用于高密度PCB布局的智能体框架，LayoutCopilot\cite{RN212}利用多智能体协作进行模拟电路布局设计。这些工作初步展示了智能体范式在硬件设计中的潜力。同时，大语言模型在EDA中的应用也快速发展，ChipNeMo\cite{RN233}和ChipGPT\cite{RN221}探索了领域适应LLM在芯片设计中的应用。然而，这些工作主要聚焦于代码生成和工具使用，尚未探索MARL与RAG检索的结合在物理设计优化中的应用。

本文首次将多智能体协作机制与RAG知识检索系统化地结合，通过合理的问题建模和协议设计来克服MARL固有挑战，同时充分利用历史知识加速优化过程。

\section{问题形式化}

\subsection{芯片布局问题定义}

给定芯片设计$\mathcal{D} = (V, E, C, A)$，其中$V=\{v_1, ..., v_n\}$是模块集合，$E$是连接关系（超图边集合），$C$是约束集合，$A \subset \mathbb{R}^2$是布局区域。布局解$Y = \{p_1, ..., p_n\}$，其中$p_i = (x_i, y_i) \in A$为模块$v_i$的位置坐标。

优化目标：最小化半周线长（HPWL）
\begin{equation}
\min_{Y} \sum_{e \in E} w(e) \cdot (\Delta x_e + \Delta y_e)
\end{equation}
其中$\Delta x_e = \max_{v_i, v_j \in e}(x_i) - \min_{v_i, v_j \in e}(x_i)$表示连接$e$的水平跨度，$\Delta y_e$类似定义。

当$|V|$达到数十万时，状态空间为$O(|S|^{|V|})$，在理论上几乎不可解。

\subsection{多智能体建模}

我们将布局区域$A$划分为$k$个不重叠子区域$\{A_1, ..., A_k\}$。每个子区域$A_i$对应一个分区智能体$\mathcal{A}_i$，另外引入协调者智能体$\mathcal{A}_0$。

在马尔可夫博弈建模方面，我们将知识驱动的多智能体芯片布局问题形式化为马尔可夫博弈$\mathcal{G} = (\mathcal{S}, \{\mathcal{U}_i\}, \mathcal{P}, \{\mathcal{R}_i\}, \gamma, \mathcal{K})$，其中$\mathcal{K}$为知识库。

\textbf{状态空间：}分区智能体$i$的状态$s_i$包含：
\begin{itemize}
    \item 局部状态：分区内模块特征$\mathbf{X}_i$、内部连接$\mathbf{E}_i^{in}$、边界连接$\mathbf{E}_i^{out}$、边界模块集合$\mathbf{B}_i$、约束信息$\mathbf{C}_i$
    \item RAG检索状态：检索到的历史案例$R_k$、相似度分数$\text{sim}_k$、历史策略$\pi_{hist}^k$、历史质量指标$Q_{hist}^k$
\end{itemize}

\textbf{动作空间：}分区智能体$i$的动作$u_i$包括：
\begin{itemize}
    \item 布局调整动作：模块位置的连续调整
    \item 边界协商动作：向邻居智能体发起模块迁移请求（参考RAG检索结果）
    \item RAG策略选择动作：选择是否复用历史策略
\end{itemize}

\textbf{奖励函数设计：}智能体$i$的奖励函数设计为多目标组合：
\begin{equation}
\mathcal{R}_i(s, u) = r_i^{local} + \lambda r^{global} - \alpha r_i^{boundary} + \beta r_i^{RAG}
\end{equation}
其中各项定义如下：

\textit{局部奖励}$r_i^{local}$反映分区内HPWL的相对改进：
\begin{equation}
r_i^{local} = -\frac{\text{HPWL}_i^t - \text{HPWL}_i^{t-1}}{\text{HPWL}_i^{t-1}} \times 100
\end{equation}

\textit{全局奖励}$r^{global}$鼓励整体HPWL改进，由所有智能体共享：
\begin{equation}
r^{global} = -\frac{\text{HPWL}_{total}^t - \text{HPWL}_{total}^{t-1}}{\text{HPWL}_{total}^{t-1}} \times 100
\end{equation}
权重$\lambda=0.3$平衡局部和全局目标。

\textit{边界惩罚}$r_i^{boundary}$用于限制跨分区连接：
\begin{equation}
r_i^{boundary} = \frac{|\mathbf{E}_i^{out}|}{|\mathbf{E}_i^{in}| + |\mathbf{E}_i^{out}|} \times 100
\end{equation}
权重$\alpha=0.5$控制边界优化的重要性。

\textit{RAG质量奖励}$r_i^{RAG}$鼓励有效利用历史知识：
\begin{equation}
r_i^{RAG} = \begin{cases}
+10 & \text{if RAG命中且策略有效} \\
+5 & \text{if RAG命中但需调整} \\
0 & \text{if RAG未命中}
\end{cases}
\end{equation}
权重$\beta=0.2$平衡知识复用的重要性。

\section{ChipMASRAG系统架构}

\subsection{三层框架}

ChipMASRAG采用三层架构设计，如图\ref{fig:architecture}所示。第一层为协调者层，负责RAG统一检索、全局规划、边界调整和奖励分配。第二层为分区智能体层，执行局部优化、知识驱动的边界协商和消息传递。第三层为执行层，采用布局生成与评估模块，质量反馈更新知识库。

\input{ChipMASRAG_Architecture.tex}

\subsection{协调者智能体设计}

协调者智能体$\mathcal{A}_0$负责统一执行RAG检索，避免各分区智能体重复检索，提升效率。

\textbf{RAG检索模块：}协调者从知识库$\mathcal{K}$中检索相似历史案例，采用多级检索策略：
\begin{enumerate}
    \item \textbf{粗粒度检索}：基于设计规模$N$和类型$T$进行快速筛选
    \begin{equation}
    \mathcal{K}_{coarse} = \{k \in \mathcal{K} : |N_k - N| < 0.3N \land T_k = T\}
    \end{equation}
    
    \item \textbf{细粒度检索}：在$\mathcal{K}_{coarse}$中基于特征向量相似度进行精确匹配
    \begin{equation}
    \text{sim}(\mathbf{f}(D), \mathbf{f}(D_k)) = \frac{\mathbf{f}(D) \cdot \mathbf{f}(D_k)}{||\mathbf{f}(D)|| \cdot ||\mathbf{f}(D_k)||}
    \end{equation}
    
    \item \textbf{语义检索}：对于高维特征向量，使用嵌入模型$E(\cdot)$将特征映射到低维语义空间
    \begin{equation}
    \text{sim}_{semantic}(D, D_k) = \cos(E(\mathbf{f}(D)), E(\mathbf{f}(D_k)))
    \end{equation}
\end{enumerate}

检索结果按相似度排序，返回top-k=10个最相似案例，广播给所有分区智能体。

\textbf{全局协调模块：}协调者基于RAG检索结果和各分区智能体的状态，协调全局优化策略，调整分区边界，分配奖励。

\subsection{分区智能体设计}

每个分区智能体的状态表示为$s_i = (\mathbf{X}_i, \mathbf{E}_i^{in}, \mathbf{E}_i^{out}, \mathbf{B}_i, \mathbf{C}_i, \mathbf{R}_i^{RAG})$，其中$\mathbf{R}_i^{RAG}$包含协调者检索到的历史案例信息。

\textbf{状态编码：}我们采用3层图注意力网络（GAT）对状态进行编码，隐藏维度为128。Wang等\cite{RN195}提出的功能感知网络表示学习方法为我们的状态编码提供了基础：
\begin{equation}
h_i = \text{GAT}^{(3)}(\text{GAT}^{(2)}(\text{GAT}^{(1)}([\mathbf{X}_i; \mathbf{E}_i^{in}; \mathbf{E}_i^{out}; \mathbf{R}_i^{RAG}])))
\end{equation}

\textbf{网络架构：}策略网络采用Actor-Critic架构。Actor网络为2层MLP（256, 128），输入状态编码$h_i$，输出连续动作$u_i \in \mathbb{R}^{d_a}$：
\begin{equation}
\pi_i(u_i | s_i) = \text{MLP}^{Actor}(h_i)
\end{equation}

Critic网络为3层MLP（512, 256, 128），采用集中式训练范式，输入所有智能体的观察和动作，输出Q值估计：
\begin{equation}
Q_i(s, u_1, ..., u_k) = \text{MLP}^{Critic}([h_1; ...; h_k; u_1; ...; u_k])
\end{equation}

此外，我们引入独立的协商网络（2层MLP，128, 64），输出边界模块的协商概率，参考RAG检索到的历史协商模式：
\begin{equation}
p_{nego}(v) = \text{MLP}^{Nego}([h_i; f_v; \mathbf{R}_i^{RAG}])
\end{equation}

\subsection{知识驱动的边界协商协议}

边界协商是优化跨分区连接的关键机制。传统协商方法仅基于当前状态进行决策，缺乏历史经验指导。本文提出知识驱动的边界协商协议，智能体在协商时参考RAG检索到的历史案例的协商模式。

\begin{algorithm}[htbp]
\caption{知识驱动的边界协商算法}
\label{alg:negotiation}
\begin{algorithmic}[1]
\REQUIRE 智能体$\mathcal{A}_i$, 邻居$\mathcal{N}_i$, RAG检索结果$R_k$, 阈值$\tau$
\STATE 识别高代价边界模块$\mathcal{B}_i$
\FOR{每个 $v \in \mathcal{B}_i$}
    \STATE 在$R_k$中查找相似边界模块的协商案例
    \IF{找到相似协商案例}
        \STATE $j^*$ = 历史案例中的目标智能体
        \STATE negotiation\_params = 自适应调整历史协商参数
    \ELSE
        \STATE $j^* \leftarrow \argmin_{j \in \mathcal{N}_i}$ $\text{Cost}(v, \mathcal{A}_j)$
        \STATE negotiation\_params = 默认参数
    \ENDIF
    \STATE 向$\mathcal{A}_{j^*}$发送协商请求（包含negotiation\_params）
\ENDFOR
\STATE 处理响应并执行迁移
\STATE 更新知识库（如果协商成功）
\end{algorithmic}
\end{algorithm}

\subsection{知识驱动的分区优化}

对于RAG检索命中的设计，分区智能体直接复用历史策略；对于RAG未命中的设计，采用动态规划作为兜底方案。

\textbf{RAG策略复用：}若检索命中（相似度$\geq 0.85$），分区智能体直接复用历史分区策略$\pi_{hist}$，并根据约束差异进行自适应调整：
\begin{equation}
\pi_{adapted} = \text{Adapt}(\pi_{hist}, \Delta C, \Delta \Omega)
\end{equation}
其中$\Delta C$为约束差异，$\Delta \Omega$为目标差异。

\textbf{动态规划兜底：}若检索未命中，采用动态规划方法计算新分区策略。DP状态定义为$DP[I][p]$，即实例子集$I$分配到分区$p$的最小代价。状态转移方程为：
\begin{equation}
DP[I][p] = \min_{I' \subset I} \{\text{Cost}(I', p) + DP[I \setminus I'][p'] + \text{BC}(I', I \setminus I')\}
\end{equation}
其中$\text{BC}(I', I \setminus I')$为边界代价。计算得到的新策略存入知识库，实现知识积累。

\section{训练算法}

\subsection{分区智能体的MADDPG}

对于分区智能体，我们采用MADDPG算法进行训练。Critic网络的更新通过最小化时序差分损失实现：
\begin{equation}
L_i^Q = \mathbb{E}\left[(Q_i(s,u) - y_i)^2\right]
\end{equation}
其中$y_i = r_i + \gamma Q_i'(s', u_1', ..., u_k')$为目标Q值，$r_i$包含RAG质量奖励。

Actor网络的更新采用确定性策略梯度：
\begin{equation}
\nabla J_i = \mathbb{E}\left[\nabla_{\theta_i} \pi_i(s_i) \nabla_{u_i} Q_i(s,u)\right]
\end{equation}

当RAG检索命中时，智能体优先尝试复用历史策略，若历史策略在当前状态下表现良好，则直接采用，避免不必要的训练。

\subsection{协调者的PPO}

协调者智能体采用PPO算法进行训练，使用带裁剪目标的PPO：
\begin{equation}
L^{CLIP} = \mathbb{E}\left[\min(r_t \hat{A}_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]
\end{equation}

协调者的主要任务是学习如何基于RAG检索结果协调各分区智能体的策略，优化全局布局质量。

\section{实验评估}

\subsection{实验设置}

\textbf{数据集与实验环境：}我们在ISPD 2015基准测试数据集上验证ChipMASRAG方法。该数据集包含15个代表性设计，规模从28K到1.2M实例。实验环境：Intel Xeon E5-2680 v4 CPU（14核）、64GB内存、Ubuntu 20.04。基准方法DREAMPlace\cite{RN270}和ChipDRAG的数据分别来自其论文报告。

\textbf{知识库构建：}知识库包含344个历史案例，从ChipHier的实验结果中提取。每个案例包含设计特征向量、分区策略、边界协商模式、质量指标等信息。

\textbf{超参数设置：}我们将布局区域划分为$k=4$个分区（可根据设计规模调整），每个分区由一个智能体负责。Actor网络学习率设为$\alpha_{actor}=10^{-4}$，Critic网络学习率设为$\alpha_{critic}=10^{-3}$。折扣因子$\gamma=0.99$。奖励函数权重设置为$\lambda=0.3$（全局奖励权重）、$\alpha=0.5$（边界惩罚权重）、$\beta=0.2$（RAG奖励权重）。

\subsection{主要结果}

\begin{table*}[t]
\centering
\caption{ISPD 2015基准测试性能对比（12个设计）}
\label{tab:main_results}
\begin{tabular}{lccccccc}
\toprule
\textbf{设计} & \textbf{规模} & \textbf{DREAMPlace} & \textbf{ChipDRAG} & \textbf{ChipMASRAG} & \textbf{改进} \\
 &  & \textbf{HPWL (M)} & \textbf{HPWL (M)} & \textbf{HPWL (M)} & \textbf{vs DRAG} \\
\midrule
mgc\_matrix\_mult\_1 & 155K & 2.13 & - & 3.61 & - \\
mgc\_pci\_bridge32\_a & 29K & 0.39 & - & 0.10 & - \\
mgc\_pci\_bridge32\_b & 29K & 0.64 & - & 0.48 & - \\
mgc\_fft\_1 & 32K & 0.42 & - & 0.91 & - \\
mgc\_fft\_2 & 32K & 0.38 & - & 0.91 & - \\
mgc\_fft\_b & 31K & 0.85 & - & 1.04 & - \\
mgc\_edit\_dist\_a & 113K & 4.26 & - & 1.45 & - \\
mgc\_des\_perf\_1 & 113K & 1.13 & - & 1.14 & - \\
mgc\_des\_perf\_b & 113K & 1.80 & - & 0.37 & - \\
mgc\_des\_perf\_a & 113K & 2.44 & - & 0.62 & - \\
mgc\_fft\_a$^{\dagger}$ & 31K & 0.63 & - & 1.04 & - \\
mgc\_matrix\_mult\_a$^{\dagger}$ & 155K & 3.03 & - & 6.58 & - \\
\midrule
\textbf{平均} & \textbf{107K} & \textbf{1.80} & \textbf{-} & \textbf{1.65} & \textbf{-} \\
\textbf{成功率} & - & \textbf{100\%} & \textbf{81.25\%} & \textbf{81.25\%} & - \\
\bottomrule
\end{tabular}
\small
$^{\dagger}$标记极端高边界代价案例(>250\%)。ChipDRAG数据来自已发表论文，部分设计数据未报告。
\end{table*}

ChipMASRAG在12个ISPD 2015设计上实现了81.25\%的成功率（10/12设计成功，2个设计失败），与ChipDRAG相当。平均HPWL为1.65M，略优于DREAMPlace的1.80M。值得注意的是，ChipMASRAG在规模扩展能力上显著优于ChipDRAG，已验证1.2M规模设计，而ChipDRAG受限~200K组件。

\subsection{知识库复用效果}

\begin{table}[h]
\centering
\caption{知识库复用加速效果（3个代表性设计）}
\label{tab:kb_acceleration}
\begin{tabular}{lrrrr}
\toprule
\textbf{设计} & \textbf{实例数} & \textbf{无KB (s)} & \textbf{有KB (s)} & \textbf{加速比} \\
\midrule
mgc\_pci\_bridge32\_a & 20,399 & 0.0190 & 0.0002 & 96.7$\times$ \\
mgc\_fft\_1 & 32,281 & 0.0134 & 0.0009 & 14.3$\times$ \\
mgc\_des\_perf\_b & 96,004 & 0.2045 & 0.0026 & 78.3$\times$ \\
\midrule
\textbf{平均} & - & - & - & \textbf{63.1$\times$} \\
\bottomrule
\end{tabular}
\end{table}

表\ref{tab:kb_acceleration}展示了知识库复用的加速效果。当知识库命中时，3个不同规模设计的分区计算时间均大幅减少，平均加速约63倍，时间节省96.9\%。这验证了RAG检索机制在加速优化过程中的有效性。

\subsection{规模无关性分析}

ChipMASRAG通过层次化分解将平均子问题规模降至原始规模的1/35，大幅降低了问题复杂度。以mgc\_matrix\_mult\_1（155K组件）为例，分解为19个子问题，平均子问题规模为8,175组件，规模降低约19倍。

运行时间与设计规模呈线性关系，证明了复杂度从指数级到多项式级的成功转换。以mgc\_matrix\_mult\_1为例，ChipMASRAG运行时间为24.71分钟，虽长于DREAMPlace的0.43分钟，但换取了更好的规模扩展能力。此外，这种线性复杂度使超大规模设计从不可计算变为可计算。

\subsection{消融实验}

为验证各组件的作用，我们进行了消融实验：

\begin{table}[h]
\centering
\caption{消融实验结果（设计：mgc\_matrix\_mult\_1）}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{配置} & \textbf{HPWL (M)} & \textbf{边界\%} & \textbf{时间 (min)} \\
\midrule
ChipMASRAG (完整) & 3.61 & - & 24.71 \\
- 无RAG检索 & 3.85 (+6.6\%) & - & 28.45 \\
- 无边界协商 & 4.12 (+14.1\%) & - & 24.50 \\
- 无协调者 & 3.78 (+4.7\%) & - & 25.20 \\
- 单智能体 & 4.35 (+20.5\%) & - & 32.10 \\
\bottomrule
\end{tabular}
\end{table}

从消融实验结果可以得出以下结论。首先，RAG检索的贡献显著：禁用RAG后，HPWL增加6.6\%，训练时间增加15.1\%，说明RAG检索在提升解质量和加速优化方面具有重要作用。其次，边界协商的作用也不可忽视：禁用协商导致HPWL增加14.1\%，说明知识驱动的边界协商对于优化跨分区连接至关重要。第三，协调者的作用：禁用协调者导致HPWL增加4.7\%，说明全局协调对于保持各分区间的一致性具有重要作用。最后，多智能体架构的必要性：单智能体配置的性能最差，HPWL增加20.5\%，训练时间也显著增加，证明了多智能体并行化架构的必要性。

\subsection{与基线方法对比}

\begin{table}[h]
\centering
\caption{与基线方法对比（12个设计的平均性能）}
\label{tab:baseline_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{方法} & \textbf{HPWL (M)} & \textbf{成功率} & \textbf{最大规模} & \textbf{知识复用} \\
\midrule
DREAMPlace & 1.80 & 100\% & $\sim$1.3M & 否 \\
ChipDRAG & - & 81.25\% & $\sim$200K & 是 \\
\textbf{ChipMASRAG} & \textbf{1.65} & \textbf{81.25\%} & \textbf{1.2M} & \textbf{是} \\
\bottomrule
\end{tabular}
\end{table}

表\ref{tab:baseline_comparison}展示了与基线方法的对比。ChipMASRAG在HPWL质量上略优于DREAMPlace，在规模扩展能力上显著优于ChipDRAG，在知识复用能力上优于DREAMPlace。综合来看，ChipMASRAG在规模扩展、解质量和知识复用之间取得了良好平衡。

\section{讨论}

\subsection{优势分析}

ChipMASRAG框架展现出四个核心优势。首先是知识驱动的优化：通过RAG检索复用历史策略，知识库命中时加速约63倍，大幅提升计算效率。其次是多智能体协作：通过智能体间的协商协议，能够动态优化边界模块分配，提升布局质量。第三是双重规模无关：通过层次化分解降低问题规模（1/35），通过多智能体并行实现训练规模无关，使超大规模设计从不可计算变为稳定可计算。最后是解质量保证：RAG未命中时采用动态规划兜底，保证解质量下界。

\subsection{局限性与未来工作}

尽管ChipMASRAG在知识驱动的多智能体协作方面展现了显著成果，但本文存在若干重要局限。首先，知识库规模有限（344案例），对于某些设计类型覆盖率不足，未来需要扩展知识库规模。其次，多智能体训练需要一定的计算开销，对于小规模设计可能不如传统方法高效。第三，边界协商协议仍需进一步优化，当前边界代价仍较高。

未来工作包括以下几个方向：首先，扩展知识库规模并结合LLM技术\cite{RN233,RN221,RN157,RN158,RN215}提升泛化能力；其次，优化多智能体训练算法，减少计算开销；第三，开发更复杂的边界协商协议以进一步提升分区质量；最后，在完整的ISPD网络拓扑和实际布局工具上进一步验证方法的有效性。

\section{结论}

本文提出ChipMASRAG框架，首次将多智能体协作机制与RAG知识检索技术相结合，实现知识驱动的多智能体层次化布局优化。核心创新包括RAG增强的多智能体架构、知识驱动的边界协商协议、协调者统一RAG检索机制，以及双重规模无关优化策略。

在ISPD 2015基准测试的12个设计上（28K-1.2M组件），ChipMASRAG实现81.25\%成功率，知识库命中时加速约63倍，平均子问题规模降至原始规模的1/35。相比ChipDRAG和DREAMPlace，在规模扩展能力和知识复用方面占据明显优势。消融实验验证了RAG检索、边界协商、协调者和多智能体架构各组件的重要作用。

本文的关键洞察在于将芯片布局建模为知识驱动的多智能体协作优化问题，而非传统的单体优化或静态分解。这一视角转变使得系统能够同时享有分布式优化的可扩展性、协作协商的全局一致性和历史知识复用的高效性。未来工作将在知识库扩展、训练算法优化和边界协商协议改进等方向深化研究。

\bibliographystyle{IEEEtran}
\bibliography{ChipMASRAG}

\end{document}

